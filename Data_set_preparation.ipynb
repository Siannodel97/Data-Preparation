{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5164779-09f8-4eb0-b398-bf0dc448a94b",
   "metadata": {},
   "source": [
    "# Data Set Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84dd765-4e51-477d-bab7-d7a4f1e81c8a",
   "metadata": {},
   "source": [
    "In this Notebook you'll learn how to prepare a basic data set where there are null values, categoric values, data scaling and normalization. This preparation is basic and essential for our model's training to get good predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36fa6a-be9c-4924-9969-ec10d7593759",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a95821-4867-496c-b925-6c820415724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Imagine we have loaded our data set as \"train_set\", a DataFrame made by Pandas from our data file in .csv format.\n",
    "\n",
    "file_path = \"...\\folder\\data_set.csv\"\n",
    "train_set = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f70fbd-b4dc-4da6-9273-6b27c06d90ff",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a07d5-7726-4ea1-b754-a0f2a98a8866",
   "metadata": {},
   "source": [
    "First of all, we are going tos separate the label from the rest of the set because we don't need to apply the same transformations in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db862f22-d887-428e-a386-5a72f9cbf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop(\"label\", axis=1) # Change \"label\" for the name of the label column\n",
    "y_train = train_set[\"label\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fa969-8359-44a9-848b-ca63e62bc454",
   "metadata": {},
   "source": [
    "Imagine our data set has null values in some features which Machine Learning algorithms can't work on. That's why exists many options to replace them like:\n",
    "* Delete corresponding rows.\n",
    "* Delete the corresponding feature (column).\n",
    "* Replace the null value with a determined value (zero, mean, ...).\n",
    "\n",
    "Let's check if exist any feature with null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69f5e0-3fcb-402b-8f46-2c0d75472763",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().any() # With .isna() you can check if there are null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dff1dd-d89d-4216-b4b9-19efcc77eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we select the rows which contains null values in\n",
    "null_values_rows = X_train[X_train.isnull().any(axis=1)]\n",
    "\n",
    "# You can see them\n",
    "null_values_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230c416-9a66-4164-9c41-774793a14af1",
   "metadata": {},
   "source": [
    "### Option 1: Delete corresponding rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d72853-4979-452a-9110-faa416e27e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the training to not change the original\n",
    "X_train_copy = X_train.copy()\n",
    "\n",
    "# Once time you know which rows has null values you can delete them\n",
    "X_train_copy.dropna(subset=[\"feature1\",\"feature2\",\"...\"], inplace=True)\n",
    "# Change the features name for those who have null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c61d59-502e-4081-81ff-1ec60c8ae707",
   "metadata": {},
   "source": [
    "We can count the number of deleted rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0370d26-9ac5-46c3-97de-3cfc98af0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of deleted rows:\",len(X_train)-len(X_train_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe52dc9-7167-4233-a4d8-c4b212fd33bb",
   "metadata": {},
   "source": [
    "### Option 2: Delete features with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175ed17-9b43-42b4-a904-370b2953b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the training to not change the original\n",
    "X_train_copy = X_train.copy()\n",
    "\n",
    "# Once time you know which features has null values you can delete them\n",
    "X_train_copy.drop([\"feature1\",\"feature2\",\"...\"], axis=1, inplace=True)\n",
    "# Change the features name for those who have null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e52e22-4565-488f-9f31-51e15b91cf00",
   "metadata": {},
   "source": [
    "We can count the number of deleted features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37af740-e89d-4a0b-a639-0c61417d26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of deleted features:\", len(list(X_train))-len(list(X_train_copy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4ddac-5cd7-49ff-b193-424b2f02f7f2",
   "metadata": {},
   "source": [
    "### Option 3: Replace the null values with a determined value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea87c4ae-3d84-4f3d-b7b9-b55b8a8248de",
   "metadata": {},
   "source": [
    "#### Mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99474a7d-738e-4bc7-b1c9-2385f6db4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the training to not change the original\n",
    "X_train_copy = X_train.copy()\n",
    "\n",
    "# Now we replace the null values with the mean of the feature's values\n",
    "mean_feature1 = X_train_copy[\"feature1\"].mean()\n",
    "mean_feature2 = X_train_copy[\"feature2\"].mean()\n",
    "\n",
    "X_train_copy[\"feature1\"] = X_train_copy[\"feature1\"].fillna(mean_feature1)\n",
    "X_train_copy[\"feature2\"] = X_train_copy[\"feature2\"].fillna(mean_feature2)\n",
    "# Change the features name for those who have null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acdc99c-8e7c-4e01-9142-9cfc4446b373",
   "metadata": {},
   "source": [
    "#### Median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68632f-ed7b-40e7-a7e5-682d68654c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the training to not change the original\n",
    "X_train_copy = X_train.copy()\n",
    "\n",
    "# Now we replace the null values with the median of the feature's values\n",
    "median_feature1 = X_train_copy[\"feature1\"].median()\n",
    "median_feature2 = X_train_copy[\"feature2\"].median()\n",
    "\n",
    "X_train_copy[\"feature1\"] = X_train_copy[\"feature1\"].fillna(median_feature1)\n",
    "X_train_copy[\"feature2\"] = X_train_copy[\"feature2\"].fillna(median_feature2)\n",
    "# Change the features name for those who have null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70c6bd-d3d5-4212-a9c8-db6511a9c950",
   "metadata": {},
   "source": [
    "### Option 3 alternative: Sklearn - SimpleImputer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a9548-c2c2-4c2d-9610-59cc4c0f4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the training to not change the original\n",
    "X_train_copy = X_train.copy()\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Imputer class doesn't allow categoric values so we need to delete them\n",
    "X_train_copy_num = X_train_copy.select_dtypes(exclude=[\"object\"])\n",
    "# Check that the subset only have numeric values\n",
    "X_train_copy_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa0a72-5d8e-414e-b16d-6f25063a85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we proporcionate the numeric values to calculate the median\n",
    "imputer.fit(X_train_copy_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0045f-f04b-48df-92f8-cfa6af1c1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the null values\n",
    "X_train_copy_num_nonan = imputer.transform(X_train_copy_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52609773-be45-4219-bcee-2acd2a13dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the result to a DataFrame from Pandas\n",
    "X_train_copy = pd.DataFrame(X_train_copy_num_nonan, columns=X_train_copy_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f24147-96ec-4633-8f3a-915e6481e117",
   "metadata": {},
   "source": [
    "## Transform categoric features to numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11daf06e-0bb1-423e-bbbf-0607068321ae",
   "metadata": {},
   "source": [
    "Before start with the transformation, let's bring the clean data set and split the features and the label in two subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0037b0-a6e2-445a-8e42-edcfaee012c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop(\"class\", axis=1)\n",
    "y_train = train_set[\"class\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40728b-a8f5-41f3-a46c-fc25c5879d08",
   "metadata": {},
   "source": [
    "Imagine our data set has plenty of categoric values so we need to transform them into numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e0cd9-87dd-4291-8f66-fe9b2b0b0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which features has categoric values (dtype = object)\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55313039-7423-4889-a96b-5cf7c261b6c5",
   "metadata": {},
   "source": [
    "There are many ways to transform categoric features into numeric values. One of them (and probably the simplest) is using the **factorize** method from Pandas which transform each categoric into sequential number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68b3de-eda6-417b-92bf-3a4a2cd24e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = X_train[\"feature1\"]\n",
    "feature1_encoded, categories = feature1.factorize()\n",
    "\n",
    "# Let's check how they has been encoded\n",
    "for i in range(10):\n",
    "    print(feature1.iloc[i], \"=\", feature1_encoded.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c901ae-2477-4e4d-b843-12f301e16f84",
   "metadata": {},
   "source": [
    "#### Advanced transformations with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e475f-9f31-4eb4-9681-56069e2fc5c2",
   "metadata": {},
   "source": [
    "##### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f8a96-404b-4f1b-ac73-50398167b639",
   "metadata": {},
   "source": [
    "It does the same codification as **factorize** from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638a8e3-84a2-4b74-95a8-d570ef039328",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = X_train[['feature1']]\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "feature1_encoded = ordinal_encoder.fit_transform(feature1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925f9a4-0b27-4ddd-9b48-a0a8ec8a541f",
   "metadata": {},
   "source": [
    "This type of codification has one problem. Certains ML algorithms which run measuring the similarity between two points by distance will consider that 1 is closer than 3 from 2. It doesn't make sense in categorical values. So it is not used for these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013edb1-7133-4e6c-aa83-c8d33fbbcdfb",
   "metadata": {},
   "source": [
    "##### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e70a32-0aa2-44ef-ac8b-d8aba8604248",
   "metadata": {},
   "source": [
    "Generate for each category from categoric feature one binary matrix which represent the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4105bea-dc9f-45a3-8eea-9f5900c8f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sparse matrix only get the positions of the values which aren't '0' to save memory\n",
    "feature1 = X_train[['feature1']]\n",
    "\n",
    "oh_encoder = OneHotEncoder()\n",
    "feature1_oh = oh_encoder.fit_transform(feature1)\n",
    "feature1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c966c8-9376-4e9c-beff-e19b2176e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sparse matrix to Numpy array\n",
    "feature1_type_oh.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c48c1d-23a6-423a-b688-6701f570a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see how it has been encoded\n",
    "for i in range(10):\n",
    "    print(feature1[\"feature1\"].iloc[i],\"=\",feature1_type_oh.toarray()[i])\n",
    "print(ordinal_enconder.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d6592-4624-40ec-b1d3-07538b71546e",
   "metadata": {},
   "source": [
    "In many cases, when partitioning the dataset or making a prediction with new examples, new values for certain categories may appear, causing an error in the **transform() function**. The **OneHotEncoding** class provides the **handle_unknown** parameter to either raise an error or ignore unknown categorical features during the transformation (the default behavior is to raise an error).\n",
    "\n",
    "When this parameter is set to \"ignore,\" and an unknown category is encountered during the transformation, the resulting encoded columns for that feature will contain only zeros. In inverse transformation, an unknown category will be denoted as None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a1467-a42b-496f-a9d2-e287a7a96595",
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_encoder = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf0e81-32d4-4299-be81-a8ae4a235b48",
   "metadata": {},
   "source": [
    "##### Get Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a217f-98ae-4d76-bfe2-16e9908c1599",
   "metadata": {},
   "source": [
    "Get Dummies is a simple method which allows to apply One-Hot Encoding to a DataFrame from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a78ea2-243b-4197-864c-df9dfa03f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X_train[\"feature1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51e6de-23d0-404f-83cc-ae987640701c",
   "metadata": {},
   "source": [
    "## Scale the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30af84-c157-461a-b949-dfd6ad5c8654",
   "metadata": {},
   "source": [
    "As same as the previous section, before commence with the scaling let's bring the clean data set and split the features and the label in two subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56e41b-f85c-4ddd-b801-832b2db9a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop(\"class\", axis=1)\n",
    "y_train = train_set[\"class\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a09f66-0d1a-46b6-9d61-4e9c98ab56cc",
   "metadata": {},
   "source": [
    "In general, Machine Learning algorithms doesn't have good predictions if the values of the input features has very different ranges. Because of that, different scaling techniques are used. It is important to note that these scaling mechanisms should not be applied to the labels.\n",
    "\n",
    "#### **Scaling Techniques**\n",
    "* **StandardScaler:** Scales data to have a mean of 0 and a standard deviation of 1.\n",
    "* **MinMaxScaler:** Scales data to a fixed range (default: 0 to 1).\n",
    "* **MaxAbsScaler:** Scales data by dividing by the maximum absolute value of each feature. Keeps data in the range [-1,1].\n",
    "* **RobustScaler:** Scales data using the median and interquartile range. Effective for datasets with outliers.\n",
    "\n",
    "#### **Normalization Techniques**\n",
    "* **Normalizer:** Re-scales each row (sample) to have a unit norm. Commonly used for text classification or clustering tasks.\n",
    "\n",
    "**It's important to do the transformations only on the train set for try these values. Later, it will be applied on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ae510-5957-4e33-a153-adc43b36b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of scaling with RobustScaler\n",
    "scale_attrs = X_train[[\"feature1\",\"feature2\",\"...\"]] # Change the names of features\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "X_train_scaled = robust_scaler.fit_transform(scale_attrs)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, colums=[\"feature1\",\"feature2\",\"...\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
